\chapter{Fundamentação Teórica}

% TODO: Atualizar isso aqui
Neste capítulo serão discutidos os aspectos relacionados à classificação de imagens de lesões de pele e detecção de melanomas. Além disso, serão explicados os conceitos
de visão computacional com redes neurais, \acp{LLM} e como estas tecnologias se integram em um \ac{MLLM}. Por fim, será discutido sobre diferentes métodos de
\textit{fine-tuning}.

\section{Lesões de Pele}

A pele é o maior órgão do corpo humano e é responsável por proteger o corpo de agentes microbiológicos, físicos e químicos. Além disso, ela também ajuda na regulação da
temperatura do corpo e, através de receptores cutâneos, proporciona informações sensoriais como o tato \cite{skin}.

Devido à exposição da pele ao ambiente, é mais comum que esse órgão sofra com doenças. As áreas afetadas são consideradas lesões de pele e podem ser usadas para
diagnósticos \cite{segmentation_skin_lesions}.

\subsection{Câncer de Pele}

O câncer é uma doença caracterizada pela multiplicação de células anormais que podem se espalhar para além do seu tecido de origem, causando tumores e levando
eventualmente à morte \cite{cancer}. O câncer de pele é o tipo mais comum da doença globalmente e é mais frequentemente causado pela exposição prolongada à radiação
ultravioleta \cite{skin_cancer}. Em geral, essa doença afeta mais a pele clara e pode afetar a mesma pessoa mais de uma vez. Uma vez desenvolvido, há um aumento de 35\%
no risco de desenvolvimento de um novo câncer de pele do mesmo tipo em um período de três anos \cite{skin_cancer_zink}.

Existem várias categorias de câncer de pele, elas podem ser agrupadas como \ac{CPNM} e melanoma. \ac{CPNM} podem ser subdivididos em \ac{CBC}, \ac{CEC}, carcinoma de
Merkel e entre outros. Essa categoria é a mais incidente, correspondendo a mais de 90\% dos cânceres diagnosticados e também é a menos fatal. O subtipo \ac{CBC} é o
mais frequente e corresponde a mais de 75\% dos casos de \ac{CPNM} no Brasil \cite{skin_cancer_zink}. O melanoma é o tipo mais fatal e menos incidente da doença. Cerca
de 75\% das mortes por câncer de pele são causadas por melanomas \cite{skin_cancer_screening}.

A doença tem um prognóstico muito melhor quando a detecção e tratamento são feitos cedo o suficiente. Segundo \textcite{skin_cancer_survival}, a taxa de sobrevivência ao
melanoma no Brasil é menor que a taxa global, sendo que há uma prevalência maior de casos avançados.

\subsection{Imagens de Lesões de Pele}

As imagens para exames dermatológicos podem ser agrupadas em diferentes categorias. Alguns exemplos baseados em procedimentos recomendados por
\textcite{fotos_dermatologia} são:

% TODO: Adicionar imagens

\begin{itemize}
      \item \textbf{Dermatoscopia}: Essas imagens são obtidas com um equipamento especializado, o dermatoscópio. Esse método permite que um diagnóstico mais preciso seja
            feito, sendo melhor que o olho nu na detecção de melanomas \cite{dermatoscopy};
      \item \textbf{Foto de aproximação com régua}: Esse tipo de imagem é obtida com uma fotografia feita a 30 centímetros da lesão, sem utilizar \textit{zoom}.
            Nessas imagens, etiquetas são colocadas próximas à lesão para auxiliar na determinação do seu tamanho;
      \item \textbf{Foto panorâmica}: Nesse método são registradas imagens de regiões do corpo, como da cabeça, tronco, braços e pernas.
\end{itemize}

\subsection{Detecção e Diagnóstico de Câncer de Pele}

O câncer de pele pode ser identificado e classificado através dos sintomas causados. Características como o tamanho da lesão, variação da cor, irregularidade do formato,
progresso ao longo do tempo e local do corpo em que a lesão se encontra são fundamentais para o diagnóstico da doença \cite{recognizing_skin_cancer}.

A análise destes sintomas é normalmente um processo visual realizado por um dermatologista, sendo que técnicas como a dermatoscopia e teledermatologia podem ser usadas.
Outros métodos não invasivos incluem, por exemplo, a análise intracutânea espectrofotométrica, ultrasonografia de alta frequência e microscopia confocal de refletância.
É possível também diagnosticar a doença através de métodos invasivos como a biópsia. Além destes métodos, há também a utilização de \ac{IA}. Soluções com \ac{IA}
utilizam imagens para a classificação da doença e podem ter uma precisão equiparável ou até maior a de dermatologistas \cite{recognizing_skin_cancer, skin_cancer_ai}.

\section{Multimodal Large Language Models}

\acp{MLLM} são \acp{IA} baseadas em \acp{LLM} que possuem a capacidade de interpretação de diferentes modalidades de informação, diferentemente de \acp{LLM}, que
operam sobre informações textuais. Esses modelos também podem ser capazes de produzir conteúdo multimodal. Essas modalidades podem ser imagens, vídeos, áudios e
entre outros \cite{mllm_survey_2023, mllm_survey_2024}.

Segundo \textcite{mllm_survey_2024}, a estrutura de um \ac{MLLM} pode ser dividida em cinco componentes, sendo eles o codificador de modalidade, projetor de entrada,
\ac{LLM}, projetor de saída e o gerador de modalidade. No caso de \acp{MLLM} que possuem apenas saída de texto, somente os três primeiros componentes estão presentes,
essa estrutura é demonstrada na \autoref{fig:mllm_structure_encoder_only}.

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.7\columnwidth,keepaspectratio]{images/mllm_structure_encoder_only.pdf}
      \caption{\small Estrutura de um \ac{MLLM} que realiza apenas a interpretação multimodal.}
      \label{fig:mllm_structure_encoder_only}
\end{figure}

Neste trabalho, o foco será dado a modelos que interpretam imagens e retornam saídas textuais. Sendo assim, a parte de geração multimodal não será coberta.

\subsection{Large Language Models}

\acp{LLM} ou Grandes Modelos de Linguagem são modelos estatísticos pré-treinados baseados em transformadores que possuem bilhões de parâmetros e conseguem processar e
gerar textos em linguagem natural \cite{llm_survey_2024}.

Embora modelos de linguagem já sejam conceitos discutidos há várias décadas, a introdução da arquitetura de transformadores no artigo \textit{Attention Is All You Need}
de \textcite{transformer} define um marco no desenvolvimento destes modelos, levando ao surgimento de \acp{LLM}.

Transformadores foram desenvolvidos inicialmente para tarefas de tradução. Porém, pouco tempo depois da sua introdução, pesquisadores da OpenAI aplicaram essa arquitetura
no desenvolvimento do modelo \ac{GPT}. O modelo demonstrou características emergentes além da simples geração de texto, como a capacidade de responder perguntas com
respostas que não estavam na sua base de treinamento \cite{gpt1}. Os modelos sucessores, \ac{GPT}-2 e \ac{GPT}-3, demonstraram ser ainda melhores em diversas tarefas,
como a tradução, resolução de problemas básicos, interpretação de texto e aritmética básica \cite{gpt2, gpt3}.

Após o lançamento comercial do \textit{ChatGPT} em 2022, vários novos \acp{LLM} foram desenvolvidos. Os mais notáveis são baseados no \ac{LLaMA} e no \ac{PaLM}
\cite{llm_survey_2024}.

As próximas seções tratarão dos conceitos da representação do texto de entrada, como a tokenização e o uso de \textit{embedding vectors} ou incorporações vetoriais.
Depois, transformadores e outros aspectos das arquiteturas de \acp{LLM} serão explicados.

\subsubsection{Tokenização}

A tokenização é um processo em que o texto é separado em partes menores, denominados \textit{tokens}. A maioria das formas de tokenização quebram palavras em
sub-palavras, isso permite que o modelo interprete novas palavras não vistas antes com base na composição das sub-palavras. Esse processo também torna o processamento de
texto mais eficiente, reduzindo a memória necessária para o vocabulário \cite{tokenizer_performance}. Na \autoref{fig:tokens} é possível ver um exemplo de texto
tokenizado. Cada preenchimento colorido denota um \textit{token}.

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.7\columnwidth,keepaspectratio]{images/tokens.png}
      \caption{\small Texto tokenizado com o tokenizador do \ac{GPT}-4o. Fonte: \textcite{tokenizer}.}
      \label{fig:tokens}
\end{figure}

O mesmo texto, representado como uma lista de \textit{IDs} de \textit{tokens} pode ser visto na \autoref{fig:tokens_ids}.

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.7\columnwidth,keepaspectratio]{images/tokens_ids.png}
      \caption{\small \textit{IDs} de \textit{tokens} gerados com o tokenizador do \ac{GPT}-4o. Fonte: \textcite{tokenizer}.}
      \label{fig:tokens_ids}
\end{figure}

\textcite{llm_survey_2024} elenca os tipos mais comuns de tokenização, sendo eles o \textit{Byte pair encoding}, \textit{Word piece encoding} e
\textit{Sentence Piece Encoding}:

\begin{itemize}
      \item \textbf{\textit{Byte pair encoding}}: Neste algoritmo o texto é quebrado com base na frequência das palavras. Palavras mais frequentes se mantém inteiras
            enquanto as menos frequentes são quebradas;
      \item \textbf{\textit{Word piece encoding}}: No \textit{Word piece encoding}, o algoritmo é inicializado com o alfabeto da linguagem e depois passa por uma etapa
            de treinamento, onde os caracteres são combinados em sub-palavras com base na frequência em que elas aparecem. O treinamento continua até que toda a base
            textual possa ser representada com o vocabulário de \textit{tokens};
      \item \textbf{\textit{Sentence Piece Encoding}}: Este método é similar aos já citados, porém, sua maior diferença está na forma em que o texto é tratado. Sendo
            que o texto é dado como uma sequência contínua de caracteres, sem assumir que delimitações existam entre as palavras. Isso se torna útil na tokenização de
            línguas sem delimitações com espaços, como, por exemplo, japonês ou chinês.
\end{itemize}

\subsubsection{Incorporação Vetorial}

Uma incorporação vetorial é um vetor de valores reais em um espaço semântico n-dimensional que representa um dado, como, por exemplo, uma palavra ou uma imagem. Esse
tipo de representação permite que diferentes tipos de dados sejam representados em um espaço vetorial unificado. Na \autoref{fig:word_embeddings} há um exemplo
simplificado da estrutura destes vetores. No contexto de \acp{LLM}, \textit{tokens} são representados com incorporações vetoriais \cite{word_embedding, mllm_survey_2023}.

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.3\columnwidth,keepaspectratio]{images/word_embeddings.pdf}
      \caption{Representação de palavras como vetores de valores reais. Fonte: \textcite{word_embedding}.}
      \label{fig:word_embeddings}
\end{figure}

Nesse espaço vetorial, direções podem ter um significado semântico. Um exemplo dado por \textcite{glove} demonstra isso com a frase \textit{"king is to queen as man is
to woman"}, que pode ser representada como \begin{math}V_{king} - V_{queen} \approx V_{man} - V_{woman}\end{math}, evidenciando que a direção do vetor
\begin{math}V_{king} - V_{queen}\end{math} codifica informações sobre gênero. A \autoref{fig:word_embeddings_directions} traz uma visualização aproximada desta relação.

\clearpage

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.6\columnwidth,keepaspectratio]{images/word_embeddings_directions.png}
      \caption{Visualização aproximada de incorporações com a direção que codifica informações de gênero evidenciada. Fonte: \textcite{word_embedding}.}
      \label{fig:word_embeddings_directions}
\end{figure}

Outro exemplo de relação semântica pode ser visto com \begin{math}V_{Paris} - V_{France} + V_{Italy} \approx V_{Rome}\end{math}, em que é possível calcular qual é a
capital da Itália somando um vetor que incorpora a relação de capital para país com um vetor que incorpora a palavra "Itália". A proximidade entre incorporações indica a
similaridade semântica entre elas \cite{word2vec_estimation}.

\subsubsection{Codificação Posicional} % TODO: Expandir a seção, falando de outras formas de codificação

Transformadores não possuem mecanismos de recorrência ou convolução, então é necessária uma forma de codificar a posição de cada \textit{token} do texto de entrada. No
modelo original de transformador, isso foi feito com a adição de vetores de codificação posicional às incorporações. Para gerar uma codificação como esta, é usada uma
combinação de funções seno e cosseno com diferentes frequências, sendo que cada dimensão da codificação corresponde a um senoide \cite{transformer}.

% TODO: Adicionar imagem

\subsubsection{Transformadores} % TODO: Comentar das variantes

O transformador é o componente central de um \ac{LLM}, ele processa o texto de entrada previamente tokenizado e representado em incorporações vetoriais com codificação
posicional e prevê o próximo \textit{token} desse texto. As explicações fornecidas neste trabalho se baseiam no modelo de \textcite{transformer}.

A arquitetura original dos transformadores possui um codificador e um decodificador, sendo que cada um deles é composto por camadas. No codificador, cada camada é
composta por duas subcamadas, sendo uma delas um mecanismo de \textit{multi-head self-attention} e outra, uma rede \textit{feed-forward}. A saída de cada uma delas possui
uma conexão residual e uma etapa de normalização. O decodificador é similar ao codificador, mas suas camadas possuem uma subcamada extra que aplica o mecanismo de
\textit{multi-head self-attention} mascarado sobre a saída do codificador. Além disso, a entrada do decodificador é deslocada em uma posição, de modo que uma nova
incorporação possa ser prevista e usada para gerar um \textit{token}. A \autoref{fig:transformer} mostra a arquitetura original de um transformador.

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.4\columnwidth,keepaspectratio]{images/transformer.png}
      \caption{Arquitetura de um transformador. Fonte: \textcite{transformer}.}
      \label{fig:transformer}
\end{figure}

O mecanismo de \textit{multi-head self-attention} permite que o modelo reconheça as relações entre diferentes partes do texto de entrada e ajuste os valores das
incorporações vetoriais com base nessas relações. Esse conceito pode ser exemplificado de forma mais intuitiva com a frase: \textit{``Você está lendo um documento, ele
foi escrito em português''}. Nesse caso, a palavra \textit{``ele''} possui uma relação com a palavra \textit{``documento''}. O mecanismo identificaria essa relação e
utilizaria essa informação para recalcular a incorporação vetorial da palavra \textit{``ele''}, de modo que ela reflita seu vínculo com \textit{``documento''}. A captura
das relações e significados é o que permite que o modelo gere mais partes do texto. Por exemplo, na frase \textit{``Um animal vivendo em seu habitat...''}, o modelo
poderia ajustar o valor semântico de uma nova incorporação com base na sua relação com outras incorporações até que ela representasse a palavra \textit{``natural''}.

O modelo de atenção proposto originalmente se chama \textit{Scaled Dot-Product Attention}. Na primeira etapa do processamento, são calculados três vetores para cada
incorporação vetorial, sendo eles o \textit{query} (\begin{math}V_Q\end{math}), \textit{key} (\begin{math}V_K\end{math}) e \textit{value} (\begin{math}V_V\end{math}).
Para realizar esta operação, as incorporações são agrupadas em uma matriz \begin{math}E\end{math} e multiplicadas pelas matrizes \begin{math}W_Q\end{math},
\begin{math}W_K\end{math} e \begin{math}W_V\end{math}, provenientes do treinamento do modelo. Assim, são obtidas as matrizes \begin{math}Q\end{math},
\begin{math}K\end{math} e \begin{math}V\end{math}. As equações abaixo ilustram este processo:

\begin{align*}
      E \times W_Q &= Q \\
      E \times W_K &= K \\
      E \times W_V &= V
\end{align*}

Além disso, os vetores nas matrizes \begin{math}Q\end{math} e \begin{math}K\end{math} possuem uma dimensionalidade menor do que os vetores de incorporação. Isso se dá
por uma escolha de desenvolvimento na arquitetura original que visava melhorar o desempenho no processamento da atenção \textit{multi-head}.

A próxima etapa se consiste no cálculo do produto escalar entre todos os vetores \begin{math}V_Q\end{math} com todos os \begin{math}V_K\end{math}, através da operação
\begin{math}Q \times K^T\end{math}. O valor resultante de cada produto indica o quanto um vetor \begin{math}V_K\end{math} atende a um \begin{math}V_Q\end{math}. Em
outras palavras, um produto escalar alto indica que o \textit{token} de onde foi calculado o \begin{math}V_K\end{math} tem uma relação com o \textit{token} de
onde foi calculado o \begin{math}V_Q\end{math}.

Os valores resultantes são divididos por \begin{math}\sqrt{d_k}\end{math}, garantindo uma maior estabilidade no gradiente de valores. Nesse caso,
\begin{math}d_k\end{math} é número de dimensões no espaço de \textit{queries} e \textit{keys}. Depois, uma função de \textit{softmax} é aplicada, normalizando os
resultados.

Por fim, os vetores da matriz \begin{math}V\end{math} são multiplicados pelos valores normalizados. Assim, vetores \begin{math}V_V\end{math} de \textit{tokens} com pouca
relação entre si serão numericamente irrelevantes. Depois, os vetores resultantes relativos a cada \textit{token} são somados, gerando novos vetores que representam o
ajuste no significado das incorporações vetoriais desses \textit{tokens}. O resultado desta etapa é uma matriz composta por esses vetores de ajuste.

O processo definido até o momento pode ser resumido com a expressão:

\begin{align*}
      Attention(Q, K, V) &= softmax({\frac{QK^T}{\sqrt{d_k}}})V
\end{align*}

O conceito de \textit{multi-head} é utilizado para melhorar a capacidade do modelo de dar atenção a diferentes partes do texto de entrada. Para cada cabeça há um
conjunto distinto de matrizes \begin{math}Q\end{math}, \begin{math}K\end{math} e \begin{math}V\end{math}. No fim, cada cabeça irá produzir um vetor de ajuste diferente
para cada \textit{token}. As matrizes de vetores de ajuste de cada cabeça são concatenadas e multiplicadas por uma matriz \begin{math}W_O\end{math}, gerando a saída da
subcamada de atenção. A expressão abaixo e a \autoref{fig:attention} demonstram o processo e a estrutura:

\begin{align*}
      MultiHead(Q, K, V) &= Concat(head_1,...,head_h)W_O
\end{align*}

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.7\columnwidth,keepaspectratio]{images/attention.png}
      \caption{Estrutura da subcamada de atenção. Fonte: \textcite{transformer}.}
      \label{fig:attention}
\end{figure}

A saída de cada subcamada possui uma conexão residual que soma os dados de entrada com os de saída dessa subcamada, aplicando uma normalização em seguida. Esse processo
atualiza as incorporações com os vetores de ajuste e também atenua o problema do desaparecimento de gradiente.

A subcamada \textit{feed-forward} é composta por duas transformações lineares com uma função de ativação \ac{ReLU} entre elas. Cada incorporação é processada
isoladamente por esta subcamada. \textcite{feed_forward_knowledge} propõe que essa rede armazena as informações factuais aprendidas no treinamento. A expressão abaixo
traz a representação matemática desta subcamada:

\begin{align*}
      FFN(x) &= ReLU(xW_1+b_1)W_2 + b_2
\end{align*}

A saída do conjunto de camadas de codificação é utilizada pelas camadas de decodificação. No decodificador há uma subcamada extra de \textit{multi-head self-attention}
mascarada que processa os dados de uma forma em que cada incorporação só possa ser atendida pelas incorporações antecessoras a ela. Em outras palavras, o valor semântico
de um \textit{token} só pode ser influenciado pelos valores dos \textit{tokens} antecessores a ele. Isso é implementado com a definição do valor
\begin{math}-\infty\end{math} para as relações ilegais após a etapa de multiplicação e escala das matrizes \begin{math}Q\end{math} e \begin{math}K\end{math}, que é
normalizado como zero com a aplicação do \textit{softmax}. Com a aplicação desta máscara e do deslocamento da entrada do codificador em uma posição, é possível prever
uma nova incorporação vetorial que dependa apenas das incorporações anteriores.

Por fim, a nova incorporação prevista pelo decodificador passa por uma transformação linear que gera um vetor de \textit{logit}. O tamanho deste vetor está associado ao
tamanho do vocabulário de \textit{tokens} do modelo, sendo que cada posição sua corresponde a um \textit{token}. O \textit{logit} é normalizado com \textit{softmax},
gerando uma distribuição de probabilidades. O \textit{token} que corresponde ao valor com a maior probabilidade é então usado para completar o texto de entrada.

\subsubsubsection{Arquiteturas de Transformadores}

O modelo explicado anteriormente possui um codificador e decodificador, sendo mais complexo e adequado para tarefas de geração de texto com base em uma entrada específica,
como a geração de resumos ou a tradução. Existem também modelos que possuem apenas codificadores ou decodificadores. Os que usam apenas codificadores são úteis para
tarefas de classificação de texto, \ac{REM} e \ac{EQA}. Modelos que usam apenas decodificadores são mais utilizados para a geração de texto, sendo que os modelos das
famílias \ac{GPT} e \ac{LLaMA} são exemplos dessa forma de implementação \cite{llm_survey_2024}.

\subsubsection{Treinamento de LLMs}

\acp{LLM} requerem grandes volumes de dados para serem treinados, sendo que esses dados textuais precisam ser tratados antes de serem usados no treinamento. O tratamento
envolve a remoção de informações falsas, textos duplicados, ruídos e de dados sem valor para o treinamento. Além disso, os vieses e desbalanceamentos também são
reduzidos nesse processo \cite{llm_survey_2024}.

\subsubsubsection{Pré-Treinamento}

O pré-treinamento é a primeira etapa de treinamento de um \ac{LLM}. Normalmente o modelo é treinado sobre um grande volume de texto não classificado de forma
auto-supervisionada. A modelagem auto-regressiva e a modelagem mascarada são duas formas comuns de realizar o pré-treinamento.

Na modelagem auto-regressiva, o modelo tenta prever o próximo \textit{token} a partir de uma sequência de \textit{tokens}. O \textit{token} previsto é comparado com o
\textit{token} esperado, assim, o modelo tentará minimizar os erros. Essa modelagem é mais adequada para \acp{LLM} que usam apenas decodificadores, devido ao fato da
predição ser baseada apenas nos \textit{tokens} antecessores ao \textit{token} previsto.

A modelagem mascarada se consiste no treinamento baseado na previsão de partes ocultas de um texto. Diferentemente da modelagem auto-regressiva, este método utiliza
todos os \textit{tokens} do texto para realizar a previsão.

\subsubsubsection{Alinhamento} \label{sec:alinhamento}

Com o pré-treinamento, os \acp{LLM} conseguem gerar e classificar textos, mas ainda é necessário alinhar o comportamento do modelo com as expectativas de seus usuários.
Com o método \ac{RLHF}, os textos gerados são avaliados por humanos, recebendo uma pontuação. Assim, é possível ajustar o modelo para que as respostas tenham
pontuações melhores. Outro método comum é o \ac{RLAIF}, que integra um modelo já alinhado ao que está em treinamento, realizando os ajustes necessários.

\subsubsubsection{Fine-Tuning} \label{sec:fine_tuning} % TODO: Expandir a seção

O \textit{fine-tuning} é uma etapa de treinamento que pode ser realizada para melhorar a qualidade das saídas sobre dados específicos ou sobre um domínio de
conhecimento.

Técnicas de \textit{fine-tuning} eficientes oferecem uma forma de adaptação de modelos com um custo computacional relativamente menor. Dentre várias
técnicas, uma das mais populares é o \ac{LoRa}. Essa técnica se consiste na introdução de matrizes treináveis menores ao \ac{LLM}, deixando as matrizes originais intactas
durante o treinamento. Assim que treinadas, essas matrizes são adicionadas às originais.

\ac{LoRa} reduz drasticamente o custo do treinamento comparado ao \textit{fine-tuning} completo, usando três vezes menos memória e em dez mil vezes menos parâmetros
durante o \textit{fine-tuning} do modelo \ac{GPT}-3 \cite{llm_survey_2023}.

É possível usar também a quantização para acelerar o \textit{fine-tuning}. A quantização se trata da redução da precisão numérica durante o treinamento. Uma técnica que
utiliza este conceito é o \ac{QLoRa}, que segundo \textcite{qlora}, foi usado no \textit{fine-tuning} de um modelo que atingiu 99.3\% do desempenho do \textit{ChatGPT}
no \textit{benchmark} Vicuna com apenas 24 horas de treinamento em uma placa de vídeo de 48 GB.

\subsection{Codificador de Modalidade}

Codificadores de modalidade são os componentes responsáveis pela extração das características mais relevantes dos dados multimodais de entrada em incorporações vetoriais.
\cite{mllm_survey_2024}.

\subsubsection{Codificador Visual}

Existem diversos codificadores visuais. Os mais comuns usam \acp{ViT} ou Transformadores Visuais nas suas arquiteturas, mas também há codificadores baseados em
convolução, como o \textit{Osprey} \cite{mllm_survey_2023}. Neste trabalho o foco será direcionado a arquiteturas baseadas em \ac{ViT}.

\subsubsubsection{Transformadores Visuais}

\acp{ViT} são transformadores capazes de processar imagens introduzidos por \textcite{dosovitskiy2020image}. A arquitetura original utiliza apenas codificadores e visa
classificar imagens.

A primeira etapa no processamento envolve a divisão da imagem em seções de menor resolução, sendo então planificadas e incorporadas. Cada incorporação resultante também
recebe uma codificação posicional simples unidimensional. Nessa etapa, uma incorporação extra também pode ser adicionada na primeira posição da entrada para objetivos de
classificação da imagem.

Na próxima etapa, as incorporações são processadas pelo transformador codificador, capturando suas relações e refinando-as. Diferentemente da arquitetura de
\textcite{transformer}, o modelo de \textcite{dosovitskiy2020image} aplica a normalização antes da subcamada de atenção. Além disso, a função de ativação usada na
subcamada de \textit{feed-forward} é a \ac{GELU}.

A classificação pode ser realizada com o processamento da incorporação de classificação por um \ac{MLP}. Assim, é gerada uma distribuição de probabilidades sobre as
classes. A \autoref{fig:vision_transformer} demonstra a estrutura de um transformador visual com os detalhes de sua implementação.

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.8\columnwidth,keepaspectratio]{images/vision_transformer.pdf}
      \caption{\small Transformador visual. Fonte: \textcite{dosovitskiy2020image}.}
      \label{fig:vision_transformer}
\end{figure}

No contexto de codificadores visuais, a classificação não é necessária, já que a saída do codificador se constitui nas incorporações processadas. Sendo assim, a
incorporação extra de classificação e o \ac{MLP} final podem ser desconsiderados \cite{dubey2024llama}.

\subsection{Projetor de Entrada}

Esse componente faz o alinhamento entre o espaço de incorporações geradas pelo codificador e o espaço de incorporações do \ac{LLM}, podendo utilizar um \ac{MLP} ou um
mecanismo de \textit{cross-attention}. A principal diferença de \textit{cross-attention} para \textit{self-attention} está no uso dos vetores de \textit{key}
provenientes do codificador para o cálculo da atenção no transformador do \ac{LLM} \cite{mllm_survey_2024}.

\subsection{Treinamento de MLLMs}

Com base no trabalho de \textcite{mllm_survey_2024}, pode-se dividir o treinamento de \acp{MLLM} em pré-treinamento e \textit{instruction-tuning}.

\subsubsection{Pré-treinamento}

O pré-treinamento de um \ac{MLLM} normalmente evolve o travamento dos codificadores de modalidade e do \ac{LLM}, fazendo com que somente o projetor seja treinado. Em
alguns casos, o codificador de modalidade também é destravado, permitindo um treinamento mais completo. O objetivo nessa etapa é alinhar o codificador de modalidade com o
\ac{LLM}. No contexto de modelos visuais, o pré-treinamento faz o uso de um grande volume de pares de imagens e textos, sendo que os textos descrevem o conteúdo das
imagens \cite{mllm_survey_2023}.

\subsubsection{Instruction-tuning} % TODO: Expandir a seção, explicando fine tuning no contexto de MLLMs

Essa etapa se consiste na melhoria do modelo para atender melhor às expectativas dos usuários e obter uma capacidade melhor na resolução de problemas envolvendo tarefas
multimodais, sendo uma etapa similar às de alinhamento e \textit{fine-tuning} discutidas na \autoref{sec:alinhamento} e \autoref{sec:fine_tuning}.

No contexto de modelos visuais, \textit{Instruction-tuning} pode ser feita inicialmente com \ac{SFT}, usando imagens e textos em um formato de pergunta e resposta, em
que uma pergunta é feita sobre a imagem. Depois da aplicação de \ac{SFT}, é possível usar \ac{RLHF} para continuar o aprimoramento do modelo \cite{mllm_survey_2024}.

\section{LLaMA 3}

A família de modelos \ac{LLaMA}-3 é composta por \acp{LLM} e \acp{MLLM} desenvolvidos pelos pesquisadores da \textit{Meta AI}. Em particular, este trabalho focará no
modelo multimodal \ac{LLaMA}-3.2 \cite{dubey2024llama}.

O \ac{LLM} base do \ac{LLaMA}-3.2 utiliza um transformador decodificador que possui algumas diferenças em relação à arquitetura original de \textcite{transformer}, as
maiores diferenças já existem desde os modelos \ac{LLaMA} e \ac{LLaMA}-2 e podem ser elencadas com base no trabalho de \textcite{touvron2023llama}:

\begin{itemize}
      \item \textbf{Pré-normalização}: As subcamadas são pré-normalizadas, em vez de serem normalizadas posteriormente;
      \item \textbf{Normalização}: A função de normalização usada é a \ac{RMSNorm};
      \item \textbf{Função de ativação}: Na subcamada de \textit{feed-forward}, a função de ativação é a \ac{SwiGLU}, em vez de \ac{ReLU};
      \item \textbf{Codificação posicional}: A codificação posicional é feita com \ac{RoPE}, em vez da codificação absoluta;
      \item \textbf{\acs{GQA}}: \ac{GQA} é utilizado é com 8 cabeças de chave-valor para melhorar a velocidade de inferência e reduzir o tamanho dos \textit{caches} de
      chave-valor durante a decodificação;
      \item \textbf{Máscara de atenção}: Uma máscara é utilizada para impedir o relacionamento entre diferentes documentos de uma mesma entrada;
      \item \textbf{Vocabulário}: O vocabulário de \textit{tokens} usado possui 128 mil \textit{tokens}.
\end{itemize}

A tabela (TODO) apresenta alguns parâmetros das variantes do \ac{LLM}.

% TODO: Adicionar tabela com as diferenças

O codificador visual utilizado é baseado no transformador visual de \textcite{dosovitskiy2020image}, sendo usada a variante \ac{ViT}-H/14 com modificações extras, como
a adição de extração de características multicamadas e de 8 camadas de \textit{gated self-attention}, deixando o codificador com um total de 850 milhões de parâmetros.

O projetor de entrada, referenciado como o adaptador de visão no trabalho de \textcite{dubey2024llama}, se baseia em \textit{cross-attention} entre as camadas de
codificação do codificador visual e as camadas de decodificação do \ac{LLM}. Mais especificamente, é aplicada uma camada de \textit{cross-attention} a cada quarta camada
do \ac{LLM}. Essas camadas também utilizam \ac{GQA} para melhorar o desempenho.

Além de imagens, o \ac{LLaMA}-3.2 também pode operar com vídeos e áudios. A \autoref{fig:llama} apresenta a arquitetura multimodal do modelo.

\begin{figure}[ht]
      \centering
      \includegraphics[width=1.0\columnwidth,keepaspectratio]{images/llama.pdf}
      \caption{\small Arquitetura multimodal de imagem e vídeo do \ac{LLaMA}-3.2. Fonte: \textcite{dubey2024llama}.}
      \label{fig:llama}
\end{figure}
