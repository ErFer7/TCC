{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from json import load, dump\n",
    "from datetime import timedelta\n",
    "\n",
    "from unsloth import FastVisionModel\n",
    "from unsloth import is_bf16_supported\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from scripts.authentication import authenticate_huggingface\n",
    "from scripts.data import LesionData, DatasetAnalysis\n",
    "from scripts.messages import create_training_message\n",
    "from scripts.training import Training\n",
    "\n",
    "import scripts.definitions as defs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autenticação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authenticate_huggingface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = '0.2'\n",
    "\n",
    "training_hyperparameters = Training(\n",
    "    base_model_name=defs.BASE_MODEL_NAME,\n",
    "    trained_model_name=defs.MODEL_NAME,\n",
    "    quantization=True,\n",
    "    prompt_type=defs.PromptType.SIMPLE_CLASSIFICATION,\n",
    "    version=version,\n",
    "    size=11,\n",
    "    peft_hyperparameters={\n",
    "        'finetune_vision_layers': True,\n",
    "        'finetune_language_layers': True,\n",
    "        'finetune_attention_modules': True,\n",
    "        'finetune_mlp_modules': True,\n",
    "        'r': 64,\n",
    "        'lora_alpha': 64,\n",
    "        'lora_dropout': 0.05,\n",
    "        'bias': 'none',\n",
    "        'random_state': 3407,\n",
    "        'use_rslora': True,\n",
    "        'loftq_config': None\n",
    "    },\n",
    "    sft_hyperparameters={\n",
    "        'seed': defs.STATIC_RANDOM_STATE,\n",
    "        'gradient_accumulation_steps': 4,\n",
    "        'per_device_train_batch_size': 1,\n",
    "        'eval_strategy': 'steps',\n",
    "        'learning_rate': 1e-4,\n",
    "        'weight_decay': 0.01,\n",
    "        'lr_scheduler_type': 'constant',\n",
    "        'bf16': is_bf16_supported(),\n",
    "        'fp16': not is_bf16_supported(),\n",
    "        'remove_unused_columns': False,\n",
    "        'optim': 'paged_adamw_32bit',\n",
    "        'report_to': 'tensorboard',\n",
    "        'logging_steps': 0.05,\n",
    "        'output_dir': 'outputs',\n",
    "        'run_name': f'training_{version}',\n",
    "        'dataset_text_field': '',\n",
    "        'dataset_kwargs': {'skip_prepare_dataset': True},\n",
    "        'dataset_num_proc': 4,\n",
    "        'max_seq_length': 2048\n",
    "    },\n",
    "    used_memory=0.0,\n",
    "    training_time=0\n",
    ")\n",
    "\n",
    "with open(join(defs.TRAINING_PATH, f'hyperparameters_{training_hyperparameters.version}.json'), 'w', encoding='utf-8') as file:\n",
    "    dump(training_hyperparameters.model_dump(), file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregamento do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(defs.DATA_PATH, 'stt_data', 'training_dataset.json'), 'r', encoding='utf-8') as file:\n",
    "    training_dataset = [LesionData(**data) for data in load(file)]\n",
    "\n",
    "with open(join(defs.DATA_PATH, 'training_dataset_analysis.json'), 'r', encoding='utf-8') as file:\n",
    "    training_dataset_analysis = DatasetAnalysis(**load(file))\n",
    "\n",
    "with open(join(defs.DATA_PATH, 'stt_data', 'validation_dataset.json'), 'r', encoding='utf-8') as file:\n",
    "    validation_dataset = [LesionData(**data) for data in load(file)]\n",
    "\n",
    "with open(join(defs.DATA_PATH, 'validation_dataset_analysis.json'), 'r', encoding='utf-8') as file:\n",
    "    validation_dataset_analysis = DatasetAnalysis(**load(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparação das mensagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_messages = []\n",
    "validation_messages = []\n",
    "\n",
    "for lesion_data in tqdm(training_dataset, desc='Criando mensagens de treinamento: '):\n",
    "    training_messages.append(create_training_message(training_hyperparameters.prompt_type,\n",
    "                                                     lesion_data,\n",
    "                                                     training_dataset_analysis))\n",
    "\n",
    "for lesion_data in tqdm(validation_dataset, desc='Criando mensagens de validação: '):\n",
    "    validation_messages.append(create_training_message(training_hyperparameters.prompt_type,\n",
    "                                                     lesion_data,\n",
    "                                                     validation_dataset_analysis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicialização do LLaMa 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    training_hyperparameters.base_model_name,\n",
    "    load_in_4bit=training_hyperparameters.quantization,\n",
    "    use_gradient_checkpointing='unsloth'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuração de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    **training_hyperparameters.peft_hyperparameters\n",
    ")\n",
    "\n",
    "FastVisionModel.for_training(model)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=UnslothVisionDataCollator(model, tokenizer),\n",
    "    train_dataset=training_messages,\n",
    "    eval_dataset=validation_messages,\n",
    "    args=SFTConfig(**training_hyperparameters.sft_hyperparameters),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f'Tempo de treinamento: {timedelta(seconds=trainer_stats.metrics[\"train_runtime\"])}')\n",
    "print(f'Memória máxima reservada: {used_memory} GB')\n",
    "\n",
    "training_hyperparameters.used_memory = used_memory\n",
    "training_hyperparameters.training_time = trainer_stats.metrics['train_runtime']\n",
    "\n",
    "with open(join(defs.TRAINING_PATH, f'hyperparameters_{training_hyperparameters.version}.json'), 'w', encoding='utf-8') as file:\n",
    "    dump(training_hyperparameters.model_dump(), file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_name = f'{training_hyperparameters.trained_model_name}-{training_hyperparameters.version}-{training_hyperparameters.size}B'\n",
    "\n",
    "if training_hyperparameters.quantization:\n",
    "    trained_model_name += '-4bit'\n",
    "\n",
    "if training_hyperparameters.prompt_type == defs.PromptType.SIMPLE_CLASSIFICATION:\n",
    "    trained_model_name += '-SC'\n",
    "\n",
    "save_path = join(defs.RESULTS_PATH, 'adapter_weights', trained_model_name)\n",
    "\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "with open(join(defs.TRAINING_PATH, 'models.json'), 'r', encoding='utf-8') as file:\n",
    "    models = {name: defs.Model(**data) for name, data in load(file)}\n",
    "\n",
    "new_model = defs.Model(\n",
    "    local=True,\n",
    "    quantized=training_hyperparameters.quantization,\n",
    "    prompt_type=training_hyperparameters.prompt_type,\n",
    "    version=training_hyperparameters.version,\n",
    "    size=training_hyperparameters.size\n",
    ")\n",
    "\n",
    "models[trained_model_name] = new_model\n",
    "\n",
    "for name, trained_model in models:\n",
    "    models[name] = trained_model.model_dump()\n",
    "\n",
    "with open(join(defs.TRAINING_PATH, 'models.json'), 'w', encoding='utf-8') as file:\n",
    "    dump(models, file, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
