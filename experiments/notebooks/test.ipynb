{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, exists\n",
    "from os import makedirs\n",
    "from json import loads, dumps\n",
    "\n",
    "from unsloth import FastVisionModel\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "\n",
    "from scripts.authentication import authenticate_huggingface\n",
    "from scripts.messages import add_inference_message, format_prompt\n",
    "\n",
    "import scripts.configuration as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edite as duas constantes abaixo\n",
    "MODEL = config.BASE_MODEL_NAME\n",
    "QUANTIZED = False  # Isso é sobreescrito no caso de modelos treinados\n",
    "TEST_ON_TRAINING = False  # Testa o modelo sobre os dados de treinamento\n",
    "\n",
    "with open(join(config.RESULTS_PATH, 'models.json'), 'r', encoding='utf-8') as file:\n",
    "    models = loads(file.read())\n",
    "\n",
    "model_stats = models[MODEL]\n",
    "model_path = ''\n",
    "\n",
    "if model_stats['local']:\n",
    "    model_path = join(config.RESULTS_PATH, 'adapter_weights', MODEL)\n",
    "else:\n",
    "    model_path = MODEL\n",
    "\n",
    "quantized = model_stats['quantized'] if model_stats['quantized'] is not None else QUANTIZED\n",
    "prompt_template = ''\n",
    "prompt_type = None\n",
    "\n",
    "match model_stats['type']:\n",
    "    case 'base' | 'simple_classification':\n",
    "        prompt_type = config.PromptType.SIMPLE_CLASSIFICATION\n",
    "        prompt_template = config.SIMPLE_CLASSIFICATION_PROMPT_TEMPLATE\n",
    "    case 'full_classification':\n",
    "        prompt_type = config.PromptType.FULL_CLASSIFICATION\n",
    "        prompt_template = config.FULL_CLASSIFICATION_PROMPT_TEMPLATE\n",
    "    case 'report':\n",
    "        prompt_type = config.PromptType.REPORT\n",
    "        prompt_template = config.REPORT_PROMPT_TEMPLATE\n",
    "    case _:\n",
    "        raise ValueError('Invalid model type')\n",
    "\n",
    "model_version = model_stats['version']\n",
    "model_size = model_stats['size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autenticação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authenticate_huggingface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregamento do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_ON_TRAINING:\n",
    "    with open(join(config.DATA_PATH, 'stt_data', 'training_dataset.json'), 'r', encoding='utf-8') as file:\n",
    "        training_dataset = loads(file.read())\n",
    "\n",
    "    with open(join(config.DATA_PATH, 'training_dataset_analysis.json'), 'r', encoding='utf-8') as file:\n",
    "        training_dataset_analysis = loads(file.read())\n",
    "\n",
    "with open(join(config.DATA_PATH, 'stt_data', 'test_dataset.json'), 'r', encoding='utf-8') as file:\n",
    "    test_dataset = loads(file.read())\n",
    "\n",
    "with open(join(config.DATA_PATH, 'test_dataset_analysis.json'), 'r', encoding='utf-8') as file:\n",
    "    test_dataset_analysis = loads(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    model_path,\n",
    "    load_in_4bit=quantized,\n",
    "    use_gradient_checkpointing='unsloth',\n",
    ")\n",
    "\n",
    "FastVisionModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparação do teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt_on_test = format_prompt(prompt_template, prompt_type, test_dataset_analysis)\n",
    "messages_on_test = add_inference_message(formatted_prompt_on_test)\n",
    "\n",
    "# Idealmente isso seria igual ao de teste\n",
    "if TEST_ON_TRAINING:\n",
    "    formatted_prompt_on_training = format_prompt(prompt_template, prompt_type, training_dataset_analysis)\n",
    "    messages_on_training = add_inference_message(formatted_prompt_on_training)\n",
    "\n",
    "test_name = f'{MODEL}_test'.strip('unsloth/')\n",
    "test_output = {'model': MODEL, 'results_on_test': [], 'results_on_training': []}\n",
    "tests_path = join(config.RESULTS_PATH, 'tests')\n",
    "\n",
    "if not exists(tests_path):\n",
    "    makedirs(tests_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes sobre os dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = tokenizer.apply_chat_template(messages_on_test, add_generation_prompt=True)\n",
    "\n",
    "for idx, exam in enumerate(tqdm(test_dataset, desc='Testing on test data: ')):\n",
    "    for image_name in exam['images']:\n",
    "        image_path = join(config.DATA_PATH, 'stt_raw_data', 'dataset', 'images', image_name)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            image,\n",
    "            input_text,\n",
    "            add_special_tokens=False,\n",
    "            return_tensors='pt',\n",
    "        ).to('cuda')\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2048,\n",
    "            use_cache=True,\n",
    "            temperature=0.01,\n",
    "            min_p=0.1\n",
    "        )\n",
    "\n",
    "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        assistant_message = result.split('assistant')[-1].strip()\n",
    "        test_output['results_on_test'].append({'exam_id': exam['id'], 'answer': assistant_message})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes sobre os dados de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_ON_TRAINING:\n",
    "    input_text = tokenizer.apply_chat_template(messages_on_test, add_generation_prompt=True)\n",
    "\n",
    "    for idx, exam in enumerate(tqdm(training_dataset, desc='Testing on training data: ')):\n",
    "        for image_name in exam['images']:\n",
    "            image_path = join(config.DATA_PATH, 'stt_raw_data', 'dataset', 'images', image_name)\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "            inputs = tokenizer(\n",
    "                image,\n",
    "                input_text,\n",
    "                add_special_tokens=False,\n",
    "                return_tensors='pt',\n",
    "            ).to('cuda')\n",
    "\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=2048,\n",
    "                use_cache=True,\n",
    "                temperature=0.01,\n",
    "                min_p=0.1\n",
    "            )\n",
    "\n",
    "            result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            assistant_message = result.split('assistant')[-1].strip()\n",
    "            test_output['results_on_training'].append({'exam_id': exam['id'], 'answer': assistant_message})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvamento do teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = join(tests_path, f'{test_name}.json')\n",
    "\n",
    "with open(test_path, 'w+', encoding='utf-8') as file:\n",
    "    file.write(dumps(test_output, indent=4, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
