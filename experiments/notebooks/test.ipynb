{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, exists\n",
    "from os import makedirs\n",
    "from json import load, dump\n",
    "from datetime import datetime\n",
    "\n",
    "from unsloth import FastVisionModel\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "\n",
    "from scripts.authentication import authenticate_huggingface\n",
    "from scripts.messages import add_inference_message, format_prompt\n",
    "\n",
    "import experiments.notebooks.scripts.definitions as defs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edite as duas constantes abaixo\n",
    "MODEL = defs.BASE_MODEL_NAME\n",
    "QUANTIZED = False  # Isso é sobreescrito no caso de modelos treinados\n",
    "TEST_ON_TRAINING = False  # Testa o modelo sobre os dados de treinamento\n",
    "DETERMINISTIC = True\n",
    "TEMPERATURE = 0.0\n",
    "\n",
    "with open(join(defs.TRAINING_PATH, 'models.json'), 'r', encoding='utf-8') as file:\n",
    "    models = load(file)\n",
    "\n",
    "model_stats = models[MODEL]\n",
    "model_path = ''\n",
    "\n",
    "if model_stats['local']:\n",
    "    model_path = join(defs.RESULTS_PATH, 'adapter_weights', MODEL)\n",
    "else:\n",
    "    model_path = MODEL\n",
    "\n",
    "quantized = model_stats['quantized'] if model_stats['quantized'] is not None else QUANTIZED\n",
    "prompt_template = ''\n",
    "prompt_type = None\n",
    "\n",
    "match model_stats['type']:\n",
    "    case 'base' | 'simple_classification':\n",
    "        prompt_type = defs.PromptType.SIMPLE_CLASSIFICATION\n",
    "        prompt_template = defs.SIMPLE_CLASSIFICATION_PROMPT_TEMPLATE\n",
    "    case 'report':\n",
    "        prompt_type = defs.PromptType.REPORT\n",
    "        prompt_template = defs.REPORT_PROMPT_TEMPLATE\n",
    "    case _:\n",
    "        raise ValueError('Invalid model type')\n",
    "\n",
    "model_version = model_stats['version']\n",
    "model_size = model_stats['size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autenticação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authenticate_huggingface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregamento do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_ON_TRAINING:\n",
    "    with open(join(defs.DATA_PATH, 'stt_data', 'training_dataset.json'), 'r', encoding='utf-8') as file:\n",
    "        training_dataset = load(file)\n",
    "\n",
    "    with open(join(defs.DATA_PATH, 'training_dataset_analysis.json'), 'r', encoding='utf-8') as file:\n",
    "        training_dataset_analysis = load(file)\n",
    "\n",
    "with open(join(defs.DATA_PATH, 'stt_data', 'test_dataset.json'), 'r', encoding='utf-8') as file:\n",
    "    test_dataset = load(file)\n",
    "\n",
    "with open(join(defs.DATA_PATH, 'test_dataset_analysis.json'), 'r', encoding='utf-8') as file:\n",
    "    test_dataset_analysis = load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    model_path,\n",
    "    load_in_4bit=quantized,\n",
    "    use_gradient_checkpointing='unsloth',\n",
    "    random_state=defs.STATIC_RANDOM_STATE\n",
    ")\n",
    "\n",
    "FastVisionModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparação do teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = format_prompt(prompt_template, prompt_type, training_dataset_analysis)\n",
    "messages = add_inference_message(formatted_prompt)\n",
    "\n",
    "test_name = f'{MODEL}_test_{datetime.now().isoformat()}'.strip('unsloth/')\n",
    "test_output = {'model_name': MODEL, 'model': models[MODEL], 'results_on_test': [], 'results_on_training': []}\n",
    "tests_path = join(defs.RESULTS_PATH, 'tests')\n",
    "\n",
    "if not exists(tests_path):\n",
    "    makedirs(tests_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes sobre os dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Verificar se isso deve ficar aqui\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "for idx, exam in enumerate(tqdm(test_dataset, desc='Testing on test data: ')):\n",
    "    image_path = join(defs.DATA_PATH, 'stt_raw_data', 'dataset', 'images', exam['image'])\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        image,\n",
    "        input_text,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors='pt',\n",
    "    ).to('cuda')\n",
    "\n",
    "    # TODO: Tentar travar o estado\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=2048,\n",
    "        use_cache=False,  # Isola melhor os casos de teste\n",
    "        do_sample=not DETERMINISTIC,  # TODO: Analisar\n",
    "        temperature=TEMPERATURE  # TODO: Analisar\n",
    "    )\n",
    "\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    assistant_message = result.split('assistant')[-1].strip()\n",
    "    structured_result = {'exam_id': exam['id'], 'image': exam['image'], 'answer': assistant_message}\n",
    "\n",
    "    test_output['results_on_test'].append(structured_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes sobre os dados de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_ON_TRAINING:\n",
    "    for idx, exam in enumerate(tqdm(training_dataset, desc='Testing on training data: ')):\n",
    "        image_path = join(defs.DATA_PATH, 'stt_raw_data', 'dataset', 'images', exam['image'])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            image,\n",
    "            input_text,\n",
    "            add_special_tokens=False,\n",
    "            return_tensors='pt',\n",
    "        ).to('cuda')\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2048,\n",
    "            use_cache=False,\n",
    "            do_sample=not DETERMINISTIC,  # TODO: Analisar\n",
    "            temperature=TEMPERATURE  # TODO: Analisar\n",
    "        )\n",
    "\n",
    "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        assistant_message = result.split('assistant')[-1].strip()\n",
    "        structured_result = {'exam_id': exam['id'], 'image': exam['image'], 'answer': assistant_message}\n",
    "\n",
    "        test_output['results_on_training'].append(structured_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvamento do teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = join(tests_path, f'{test_name}.json')\n",
    "\n",
    "with open(test_path, 'w+', encoding='utf-8') as file:\n",
    "    dump(test_output, file, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
