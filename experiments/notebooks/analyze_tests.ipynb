{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise dos testes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from json import load, dump\n",
    "from copy import deepcopy\n",
    "\n",
    "import scripts.definitions as defs\n",
    "import scripts.analysis as analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_NAME = 'Llama-3.2-11B-Vision-Instruct_test.json'\n",
    "PLOT_TITLE = 'LLaMA-3.2-11B-Vision-Instruct (Quantizado)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregamento dos testes e dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(defs.RESULTS_PATH, 'tests', TEST_NAME), 'r', encoding='utf-8') as file:\n",
    "    test = load(file)\n",
    "\n",
    "with open(join(defs.DATA_PATH, 'stt_data', 'test_dataset.json'), 'r', encoding='utf-8') as file:\n",
    "    test_dataset = load(file)\n",
    "\n",
    "with open(join(defs.DATA_PATH, 'stt_data', 'training_dataset.json'), 'r', encoding='utf-8') as file:\n",
    "    training_dataset = load(file)\n",
    "\n",
    "with open(join(defs.DATA_PATH, 'test_dataset_analysis.json'), 'r', encoding='utf-8') as file:\n",
    "    test_dataset_analysis = load(file)\n",
    "\n",
    "with open(join(defs.DATA_PATH, 'training_dataset_analysis.json'), 'r', encoding='utf-8') as file:\n",
    "    training_dataset_analysis = load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processamento dos testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanitized_results_on_test = analysis.structure_answers(test['model']['type'], test['results_on_test'])\n",
    "sanitized_results_on_training = analysis.structure_answers(test['model']['type'], test['results_on_training'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvamento dos testes processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_test = {\n",
    "    'model_name': test['model_name'],\n",
    "    'model': test['model'],\n",
    "    'valid_results_on_test': [],\n",
    "    'valid_results_on_training': [],\n",
    "    'invalid_results_on_test': [],\n",
    "    'invalid_results_on_training': []\n",
    "}\n",
    "\n",
    "valid_results_on_test = list(filter(lambda x: x['answer']['valid'], sanitized_results_on_test))\n",
    "invalid_results_on_test = list(filter(lambda x: not x['answer']['valid'], sanitized_results_on_test))\n",
    "\n",
    "print(f'Resultados válidos para testes sobre dados de teste: {len(sanitized_results_on_test) - len(invalid_results_on_test)}')\n",
    "print(f'Resultados válidos para testes sobre dados de teste: {len(invalid_results_on_test)}')\n",
    "\n",
    "valid_results_on_training = list(filter(lambda x: x['answer']['valid'], sanitized_results_on_training))\n",
    "invalid_results_on_training = list(filter(lambda x: not x['answer']['valid'], sanitized_results_on_training))\n",
    "\n",
    "print(\n",
    "    f'Resultados válidos para testes sobre dados de treinamento: {len(sanitized_results_on_test) - len(invalid_results_on_training)}')\n",
    "print(f'Resultados válidos para testes sobre dados de treinamento: {len(invalid_results_on_training)}')\n",
    "\n",
    "result_lists = {\n",
    "    'valid_results_on_test': valid_results_on_test,\n",
    "    'valid_results_on_training': valid_results_on_training,\n",
    "    'invalid_results_on_test': invalid_results_on_test,\n",
    "    'invalid_results_on_training': invalid_results_on_training\n",
    "}\n",
    "\n",
    "for key, result_list in result_lists:\n",
    "    for result in result_list:\n",
    "        result_copy = deepcopy(result)\n",
    "\n",
    "        del result_copy['answer']['valid']\n",
    "\n",
    "        structured_test[key].append(result_copy)\n",
    "\n",
    "with open(join(defs.RESULTS_PATH, 'tests', TEST_NAME.replace('.json', '_structured.json')), 'w', encoding='utf-8') as file:\n",
    "    dump(structured_test, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associação de pares com as respostas corretas para dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fazer o mesmo para dados de treinamento\n",
    "\n",
    "test_dataset_dict = {exam['exam_id']: exam for exam in test_dataset}\n",
    "result_pairs = []\n",
    "\n",
    "if test['model']['type'] == 'report':\n",
    "    for result in sanitized_results_on_test:\n",
    "        if result['exam_id'] in test_dataset_dict:\n",
    "            result_answer = {}\n",
    "\n",
    "            if result['answer']['valid']:\n",
    "                del result['answer']['valid']\n",
    "                result_answer = result['answer']\n",
    "            else:\n",
    "                result_answer = {\n",
    "                    'elementary_lesions': ['unclear'],\n",
    "                    'secondary_lesions': ['unclear'],\n",
    "                    'coloration': ['unclear'],\n",
    "                    'morphology': ['unclear'],\n",
    "                    'size': 'unclear',\n",
    "                    'skin_lesion': 'unclear',\n",
    "                    'risk': 'unclear'\n",
    "                }\n",
    "\n",
    "            report = test_dataset_dict[result['exam_id']]\n",
    "\n",
    "            expected_answer = {\n",
    "                'elementary_lesions': report['elementary_lesions'],\n",
    "                'secondary_lesions': report['secondary_lesions'],\n",
    "                'coloration': report['coloration'],\n",
    "                'morphology': report['morphology'],\n",
    "                'size': report['size'],\n",
    "                'skin_lesion': report['skin_lesion'],\n",
    "                'risk': report['risk']\n",
    "            }\n",
    "\n",
    "            result_pair = {\n",
    "                'answer': result_answer,\n",
    "                'expected': expected_answer\n",
    "            }\n",
    "\n",
    "            result_pairs.append(result_pair)\n",
    "else:\n",
    "    for result in sanitized_results_on_test:\n",
    "        if result['exam_id'] in test_dataset_dict:\n",
    "            result_answer = ''\n",
    "\n",
    "            if result['answer']['valid']:\n",
    "                del result['answer']['valid']\n",
    "                result_answer = result['answer']\n",
    "            else:\n",
    "                result_answer = 'unclear'\n",
    "\n",
    "            expected_answer = test_dataset_dict[result['exam_id']]['skin_lesion']\n",
    "\n",
    "            result_pair = {\n",
    "                'answer': result_answer,\n",
    "                'expected': expected_answer\n",
    "            }\n",
    "\n",
    "            result_pairs.append(result_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TODO\n",
    "if test['model']['type'] == 'report':\n",
    "    pass\n",
    "else:\n",
    "    pass\n",
    "\n",
    "# TODO: Fix\n",
    "accuracy = analysis.create_confusion_matrix(pairs, model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
