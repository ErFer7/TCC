{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise dos testes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "from os.path import join\n",
    "from json import load, dump\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk import download\n",
    "from rouge_score.rouge_scorer import RougeScorer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scripts.test import Test\n",
    "from scripts.data import SimpleLesionData, SimpleDatasetAnalysis, get_skin_lesions_numeric_labels_dict, get_risk_labels_dict\n",
    "\n",
    "import scripts.definitions as defs\n",
    "import scripts.analysis as analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_NAME = 'LLaDerm-0.21-11B_test_2025-04-16T15_10_27.906580.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregamento dos testes e dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(defs.RESULTS_PATH, 'tests', TEST_NAME), 'r', encoding='utf-8') as file:\n",
    "    test = Test(**load(file))\n",
    "\n",
    "with open(join(defs.DATA_PATH, 'stt_data', 'test_dataset.json'), 'r', encoding='utf-8') as file:\n",
    "    test_dataset = [SimpleLesionData(**data) for data in load(file)]\n",
    "\n",
    "with open(join(defs.DATA_PATH, 'stt_data', 'training_dataset.json'), 'r', encoding='utf-8') as file:\n",
    "    training_dataset = [SimpleLesionData(**data) for data in load(file)]\n",
    "\n",
    "with open(join(defs.DATA_PATH, 'simple_dataset_analysis.json'), 'r', encoding='utf-8') as file:\n",
    "    simple_dataset_analysis = SimpleDatasetAnalysis(**load(file))\n",
    "\n",
    "with open(join(defs.DATA_PATH, 'test_dataset_analysis.json'), 'r', encoding='utf-8') as file:\n",
    "    test_dataset_analysis = SimpleDatasetAnalysis(**load(file))\n",
    "\n",
    "with open(join(defs.DATA_PATH, 'training_dataset_analysis.json'), 'r', encoding='utf-8') as file:\n",
    "    training_dataset_analysis = SimpleDatasetAnalysis(**load(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processamento dos testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_results_on_test = test.results_on_test_data\n",
    "simple_results_on_training = test.results_on_training_data\n",
    "\n",
    "structured_classification_results_on_test = analysis.structure_classification_results(test.model.prompt_type, test.results_on_test_data)\n",
    "structured_classification_results_on_training = analysis.structure_classification_results(test.model.prompt_type, test.results_on_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvamento dos testes processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_results_on_test = list(filter(lambda x: x.answer.valid, structured_classification_results_on_test))\n",
    "invalid_results_on_test = list(filter(lambda x: not x.answer.valid, structured_classification_results_on_test))\n",
    "\n",
    "print(f'Resultados válidos para testes sobre dados de teste: {len(valid_results_on_test)}')\n",
    "print(f'Resultados inválidos para testes sobre dados de teste: {len(invalid_results_on_test)}')\n",
    "\n",
    "valid_results_on_training = list(filter(lambda x: x.answer.valid, structured_classification_results_on_training))\n",
    "invalid_results_on_training = list(filter(lambda x: not x.answer.valid, structured_classification_results_on_training))\n",
    "\n",
    "print(f'Resultados válidos para testes sobre dados de treinamento: {len(valid_results_on_training)}')\n",
    "print(f'Resultados inválidos para testes sobre dados de treinamento: {len(invalid_results_on_training)}')\n",
    "\n",
    "test_analysis = analysis.TestAnalysis(\n",
    "    test_name=TEST_NAME,\n",
    "    valid_results_on_test_data=valid_results_on_test,\n",
    "    valid_results_on_training_data=valid_results_on_training,\n",
    "    invalid_results_on_test_data=invalid_results_on_test,\n",
    "    invalid_results_on_training_data=invalid_results_on_training\n",
    ")\n",
    "\n",
    "makedirs(join(defs.RESULTS_PATH, 'tests', 'analysis'), exist_ok=True)\n",
    "\n",
    "with open(join(defs.RESULTS_PATH, 'tests', 'analysis', TEST_NAME.replace('.json', '_analysis.json')), 'w', encoding='utf-8') as file:\n",
    "    dump(test_analysis.model_dump(), file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associação de pares com as respostas corretas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_result_pair_on_test = analysis.associate_simple_results_with_data(test_dataset,\n",
    "                                                                         test.model.prompt_type,\n",
    "                                                                         simple_results_on_test)\n",
    "\n",
    "simple_result_pair_on_training = analysis.associate_simple_results_with_data(training_dataset,\n",
    "                                                                             test.model.prompt_type,\n",
    "                                                                             simple_results_on_training)\n",
    "\n",
    "result_pairs_on_test = analysis.associate_classification_results_with_data(test_dataset,\n",
    "                                                                           test.model.prompt_type,\n",
    "                                                                           structured_classification_results_on_test)\n",
    "\n",
    "result_pairs_on_training = analysis.associate_classification_results_with_data(\n",
    "    training_dataset, test.model.prompt_type, structured_classification_results_on_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanitização dos pares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skin_lesion_label_pairs_on_test = analysis.get_label_pairs(result_pairs_on_test, 'skin_lesion')\n",
    "skin_lesion_label_pairs_on_training = analysis.get_label_pairs(result_pairs_on_training, 'skin_lesion')\n",
    "risk_label_pairs_on_test = analysis.get_label_pairs(result_pairs_on_test, 'risk')\n",
    "risk_label_pairs_on_training = analysis.get_label_pairs(result_pairs_on_training, 'risk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversão das labels de lesões de pele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skin_lesions_numeric_labels_dict = get_skin_lesions_numeric_labels_dict(simple_dataset_analysis)\n",
    "risk_labels_dict = get_risk_labels_dict(simple_dataset_analysis)\n",
    "\n",
    "DEFAULT_LABEL = 'I'\n",
    "\n",
    "numeric_skin_lesion_pairs_on_test = []\n",
    "numeric_skin_lesion_pairs_on_training = []\n",
    "risk_pairs_on_test = []\n",
    "risk_pairs_on_training = []\n",
    "\n",
    "for pair in skin_lesion_label_pairs_on_test:\n",
    "    expected = skin_lesions_numeric_labels_dict.get(pair[0], DEFAULT_LABEL)\n",
    "    predicted = skin_lesions_numeric_labels_dict.get(pair[1], DEFAULT_LABEL)\n",
    "\n",
    "    numeric_skin_lesion_pairs_on_test.append((expected, predicted))\n",
    "\n",
    "for pair in risk_label_pairs_on_test:\n",
    "    expected = risk_labels_dict.get(pair[0], DEFAULT_LABEL)\n",
    "    predicted = risk_labels_dict.get(pair[1], DEFAULT_LABEL)\n",
    "\n",
    "    risk_pairs_on_test.append((expected, predicted))\n",
    "\n",
    "for pair in skin_lesion_label_pairs_on_training:\n",
    "    expected = skin_lesions_numeric_labels_dict.get(pair[0], DEFAULT_LABEL)\n",
    "    predicted = skin_lesions_numeric_labels_dict.get(pair[1], DEFAULT_LABEL)\n",
    "\n",
    "    numeric_skin_lesion_pairs_on_training.append((expected, predicted))\n",
    "\n",
    "for pair in risk_label_pairs_on_training:\n",
    "    expected = risk_labels_dict.get(pair[0], DEFAULT_LABEL)\n",
    "    predicted = risk_labels_dict.get(pair[1], DEFAULT_LABEL)\n",
    "\n",
    "    risk_pairs_on_training.append((expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise dos testes sobre os dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = test.tested_model\n",
    "quantized = '(Quantizado)' if test.model.quantized else ''\n",
    "\n",
    "if len(result_pairs_on_test) > 0:\n",
    "    print(f'Acurácia - Lesões de pele: {analysis.calculate_accuracy(numeric_skin_lesion_pairs_on_test) * 100.0:.1f}%')\n",
    "    print(f'Precisão - Lesões de pele: {analysis.calculate_precision(numeric_skin_lesion_pairs_on_test) * 100.0:.1f}%')\n",
    "    print(f'Recall - Lesões de pele: {analysis.calculate_recall(numeric_skin_lesion_pairs_on_test) * 100.0:.1f}%')\n",
    "    print(f'F1 - Lesões de pele: {analysis.calculate_f1(numeric_skin_lesion_pairs_on_test) * 100.0:.1f}%')\n",
    "\n",
    "    print(f'Acurácia - Risco: {analysis.calculate_accuracy(risk_pairs_on_test) * 100.0:.1f}%')\n",
    "    print(f'Precisão - Risco: {analysis.calculate_precision(risk_pairs_on_test) * 100.0:.1f}%')\n",
    "    print(f'Recall - Risco: {analysis.calculate_recall(risk_pairs_on_test) * 100.0:.1f}%')\n",
    "    print(f'F1 - Risco: {analysis.calculate_f1(risk_pairs_on_test) * 100.0:.1f}%')\n",
    "\n",
    "    skin_lesion_classes = list(map(lambda x: x[1], skin_lesions_numeric_labels_dict.items())) + [DEFAULT_LABEL]\n",
    "\n",
    "    analysis.create_confusion_matrix(numeric_skin_lesion_pairs_on_test,  # type: ignore\n",
    "                                     skin_lesion_classes,\n",
    "                                     f'{model_name} {quantized} - Lesões de pele',\n",
    "                                     join(defs.RESULTS_PATH, 'plots', f'skin_lesions_{TEST_NAME[:-4]}'),\n",
    "                                     True,\n",
    "                                     '.1f')\n",
    "\n",
    "    if test.model.prompt_type == defs.PromptType.REPORT:\n",
    "        risk_classes = list(map(lambda x: x[1], risk_labels_dict.items())) + [DEFAULT_LABEL]\n",
    "\n",
    "        risk_accuracy = analysis.create_confusion_matrix(risk_pairs_on_test,  # type: ignore\n",
    "                                                         risk_classes,\n",
    "                                                         f'{model_name} {quantized} - Classificação de risco',\n",
    "                                                         join(defs.RESULTS_PATH, 'plots', f'risk_{TEST_NAME[:-4]}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise dos testes sobre os dados de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(result_pairs_on_training) > 0:\n",
    "    print(f'Acurácia - Lesões de pele: {analysis.calculate_accuracy(numeric_skin_lesion_pairs_on_training) * 100.0:.1f}%')\n",
    "    print(f'Precisão - Lesões de pele: {analysis.calculate_precision(numeric_skin_lesion_pairs_on_training) * 100.0:.1f}%')\n",
    "    print(f'Recall - Lesões de pele: {analysis.calculate_recall(numeric_skin_lesion_pairs_on_training) * 100.0:.1f}%')\n",
    "    print(f'F1 - Lesões de pele: {analysis.calculate_f1(numeric_skin_lesion_pairs_on_training) * 100.0:.1f}%')\n",
    "\n",
    "    print(f'Acurácia - Risco: {analysis.calculate_accuracy(risk_pairs_on_training) * 100.0:.1f}%')\n",
    "    print(f'Precisão - Risco: {analysis.calculate_precision(risk_pairs_on_training) * 100.0:.1f}%')\n",
    "    print(f'Recall - Risco: {analysis.calculate_recall(risk_pairs_on_training) * 100.0:.1f}%')\n",
    "    print(f'F1 - Risco: {analysis.calculate_f1(risk_pairs_on_training) * 100.0:.1f}%')\n",
    "\n",
    "    analysis.create_confusion_matrix(numeric_skin_lesion_pairs_on_training,  # type: ignore\n",
    "                                     skin_lesion_classes,\n",
    "                                     f'{model_name} {quantized} - Lesões de pele',\n",
    "                                     join(defs.RESULTS_PATH, 'plots', f'skin_lesions_{TEST_NAME[:-4]}'),\n",
    "                                     True,\n",
    "                                     '.1f')\n",
    "\n",
    "    if test.model.prompt_type == defs.PromptType.REPORT:\n",
    "        risk_accuracy = analysis.create_confusion_matrix(risk_pairs_on_training,  # type: ignore\n",
    "                                                         risk_classes,\n",
    "                                                         f'{model_name} {quantized} - Classificação de risco',\n",
    "                                                         join(defs.RESULTS_PATH, 'plots', f'risk_{TEST_NAME[:-4]}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise estatística dos textos sobre os dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download('wordnet')\n",
    "\n",
    "bleu_scores = []\n",
    "rouge1_precision = []\n",
    "rouge1_recall = []\n",
    "rouge1_fmeasure = []\n",
    "rougeL_precision = []\n",
    "rougeL_recall = []\n",
    "rougeL_fmeasure = []\n",
    "meteor_scores = []\n",
    "\n",
    "rouge_scorer = RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "for simple_result_pair in tqdm(simple_result_pair_on_test, desc='Analisando resultados: '):\n",
    "    expected = simple_result_pair.expected\n",
    "    answer = simple_result_pair.answer\n",
    "\n",
    "    bleu_score = sentence_bleu([expected.split()], answer.split(), smoothing_function=SmoothingFunction().method4)\n",
    "\n",
    "    bleu_scores.append(bleu_score)\n",
    "\n",
    "    rouge_scores = rouge_scorer.score(expected, answer)\n",
    "\n",
    "    precision, recall, fmeasure = rouge_scores['rouge1']\n",
    "\n",
    "    rouge1_precision.append(precision)\n",
    "    rouge1_recall.append(recall)\n",
    "    rouge1_fmeasure.append(fmeasure)\n",
    "\n",
    "    precision, recall, fmeasure = rouge_scores['rougeL']\n",
    "\n",
    "    rougeL_precision.append(precision)\n",
    "    rougeL_recall.append(recall)\n",
    "    rougeL_fmeasure.append(fmeasure)\n",
    "\n",
    "    meteor_score_value = meteor_score([expected.split()], answer.split())\n",
    "\n",
    "    meteor_scores.append(meteor_score_value)\n",
    "\n",
    "\n",
    "print(f'BLEU-4: {sum(bleu_scores) / len(bleu_scores):.2%}')\n",
    "print(f'ROUGE-1 precision: {sum(rouge1_precision) / len(rouge1_precision):.2%}')\n",
    "print(f'ROUGE-1 recall: {sum(rouge1_recall) / len(rouge1_recall):.2%}')\n",
    "print(f'ROUGE-1 fmeasure: {sum(rouge1_fmeasure) / len(rouge1_fmeasure):.2%}')\n",
    "print(f'ROUGE-L precision: {sum(rougeL_precision) / len(rougeL_precision):.2%}')\n",
    "print(f'ROUGE-L recall: {sum(rougeL_recall) / len(rougeL_recall):.2%}')\n",
    "print(f'ROUGE-L fmeasure: {sum(rougeL_fmeasure) / len(rougeL_fmeasure):.2%}')\n",
    "print(f'METEOR: {sum(meteor_scores) / len(meteor_scores):.2%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
