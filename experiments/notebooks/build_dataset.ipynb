{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construção do dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from os import makedirs\n",
    "from json import load, dump\n",
    "from re import search\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import scripts.definitions as defs\n",
    "import scripts.data as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtragem dos IDs de exames de aproximação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd35ae475c45479b94958da7ef26ec8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando exames:   0%|          | 0/3651 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(join(defs.DATA_PATH, 'stt_raw_data', 'dataset', 'dataset.json'), 'r', encoding='utf-8') as file:\n",
    "    dataset = [dt.RawData(**data) for data in load(file)]\n",
    "\n",
    "raw_lesion_dataset: dict[int, list[dt.RawLesionData]] = {}\n",
    "\n",
    "for raw_data in tqdm(dataset, desc='Processando exames: '):\n",
    "    approximation_series = list(filter(lambda series: search(r'LesÃ£o \\d+.', series.seriesdescription),\n",
    "                                       raw_data.series))\n",
    "\n",
    "    if len(approximation_series) == 0:\n",
    "        continue\n",
    "\n",
    "    raw_lesion_dataset[raw_data.id_exame] = []\n",
    "\n",
    "    for series in approximation_series:\n",
    "        series_description = series.seriesdescription.split()\n",
    "        lesion_location = ' '.join(series_description[3:]).encode('latin1').decode('utf-8')\n",
    "\n",
    "        raw_lesion_data = dt.RawLesionData(\n",
    "            exam_id=raw_data.id_exame,\n",
    "            images=series.instances,\n",
    "            lesion_number=int(series_description[1]),\n",
    "            lesion_location=lesion_location,\n",
    "            report=''\n",
    "        )\n",
    "\n",
    "        raw_lesion_dataset[raw_data.id_exame].append(raw_lesion_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversão de CSV para JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9ef3864cfb4a70961946c0fb1eed22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando laudos: : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(join(defs.DATA_PATH, 'stt_raw_data', 'REDE_QUALIDADE-laudos-reemitidos.csv'))\n",
    "\n",
    "REPLACEMENTS = {\n",
    "    '\\\\n': '\\n',\n",
    "    '<br />': '\\n',\n",
    "    '&emsp;': ' ',\n",
    "    '&lt;': '<',\n",
    "    '&gt;': '>',\n",
    "    '–': '-',\n",
    "}\n",
    "\n",
    "raw_report_dataset = {}\n",
    "\n",
    "for _, raw_report in tqdm(df.iterrows(), desc='Processando laudos: '):\n",
    "    raw_report = dt.RawReport(**raw_report.to_dict())\n",
    "\n",
    "    exam_id = int(raw_report.id_exame)\n",
    "\n",
    "    if exam_id not in raw_lesion_dataset:\n",
    "        continue\n",
    "\n",
    "    report = raw_report.laudo\n",
    "\n",
    "    for pattern, replacement in REPLACEMENTS.items():\n",
    "        report = report.replace(pattern, replacement)\n",
    "\n",
    "    raw_report_dataset[exam_id] = report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geração dos laudos estruturados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05126ad75b044de93d7d70f7012494e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estruturando laudos:   0%|          | 0/3651 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reports = {}\n",
    "\n",
    "for exam_id, raw_report in tqdm(raw_report_dataset.items(), desc='Estruturando laudos: '):\n",
    "    # O [1:] remove o tipo de laudo. Sempre é \"Exame de Teledermatologia\"\n",
    "    report_parts = list(map(str.strip, raw_report.splitlines()))[1:]\n",
    "\n",
    "    parsed_reports = []\n",
    "    footnotes = {}\n",
    "\n",
    "    while True:\n",
    "        report, has_next = dt.parse_report(report_parts)\n",
    "\n",
    "        if not has_next:\n",
    "            footnotes = dt.parse_report_footnote(report_parts)\n",
    "            break\n",
    "\n",
    "        parsed_reports.append(report)\n",
    "\n",
    "    locations: dict[str, list[dt.Report]] = {}\n",
    "\n",
    "    for parsed_report in parsed_reports:\n",
    "        if parsed_report.location not in locations:\n",
    "            locations[parsed_report.location] = []\n",
    "\n",
    "        locations[parsed_report.location].append(parsed_report)\n",
    "\n",
    "    for location, reports in locations.items():\n",
    "        valid = True\n",
    "        reference_report = reports[0]\n",
    "\n",
    "        for report in reports[1:]:\n",
    "            if report.elementary_lesions != reference_report.elementary_lesions or \\\n",
    "               report.secondary_lesions != reference_report.secondary_lesions or \\\n",
    "               report.coloration != reference_report.coloration or \\\n",
    "               report.morphology != reference_report.morphology or \\\n",
    "               report.size != reference_report.size or \\\n",
    "               report.distribution != reference_report.distribution or \\\n",
    "               report.risk != reference_report.risk or \\\n",
    "               report.skin_lesion != reference_report.skin_lesion:\n",
    "                valid = False\n",
    "                break\n",
    "\n",
    "        if not valid:\n",
    "            for report in reports:\n",
    "                parsed_reports.remove(report)\n",
    "            continue\n",
    "\n",
    "    for raw_lesion_data in raw_lesion_dataset[exam_id]:\n",
    "        index = 0\n",
    "\n",
    "        while index < len(parsed_reports):\n",
    "            report = parsed_reports[index]\n",
    "\n",
    "            if raw_lesion_data.lesion_location == report.location:\n",
    "                if raw_lesion_data.report != '':\n",
    "                    raise ValueError('Conflito de laudos')\n",
    "\n",
    "                raw_lesion_data.report = parsed_reports.pop(index)\n",
    "                break\n",
    "\n",
    "            index += 1\n",
    "        \n",
    "        if raw_lesion_data.report != '':\n",
    "            lesion_number = raw_lesion_data.lesion_number\n",
    "            footnote = footnotes.get(lesion_number, None)\n",
    "\n",
    "            if footnote is not None:\n",
    "                raw_lesion_data.report.skin_lesion_conclusion += footnote  # type: ignore\n",
    "\n",
    "dataset: list[dt.LesionData] = []\n",
    "\n",
    "for exam_id, raw_lesion_datalist in raw_lesion_dataset.items():\n",
    "    for raw_lesion_data in raw_lesion_datalist:\n",
    "        if raw_lesion_data.report != '':\n",
    "            for image in raw_lesion_data.images:\n",
    "                lesion_data = dt.LesionData(exam_id=exam_id, image=image, report=raw_lesion_data.report)  # type: ignore\n",
    "                dataset.append(lesion_data)\n",
    "\n",
    "approximation_exams_dicts = [approximation_exam.model_dump() for approximation_exam in dataset]\n",
    "dataset_name = 'dataset.json'\n",
    "\n",
    "makedirs(join(defs.DATA_PATH, 'stt_data'), exist_ok=True)\n",
    "\n",
    "with open(join(defs.DATA_PATH, 'stt_data', dataset_name), 'w', encoding='utf-8') as file:\n",
    "    dump(approximation_exams_dicts, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "data_analysis = dt.analyse_dataset(dataset, defs.DATA_PATH, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processamento das imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makedirs(join(defs.DATA_PATH, 'stt_data', 'images'), exist_ok=True)\n",
    "\n",
    "for data in tqdm(dataset, desc='Processando imagens: '):\n",
    "    image_path = join(defs.DATA_PATH, 'stt_raw_data', 'dataset', 'images', data.image)\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert('RGB')\n",
    "\n",
    "    largest_side = max(image.size)\n",
    "\n",
    "    if largest_side > defs.MAX_IMAGE_SIZE:\n",
    "        scale = defs.MAX_IMAGE_SIZE / largest_side\n",
    "        new_size = tuple(int(dimension * scale) for dimension in image.size)\n",
    "        image = image.resize(new_size)  # type: ignore\n",
    "\n",
    "    image.save(join(defs.DATA_PATH, 'stt_data', 'images', data.image), format='JPEG', quality=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção de lesões raras e de dados desnecessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = list(filter(lambda data: data_analysis.skin_lesion_distribution.classes[data.report.skin_lesion].count >= 20,\n",
    "                      dataset))\n",
    "\n",
    "simple_dataset = []\n",
    "\n",
    "for data in dataset:\n",
    "    simple_report = dt.SimpleReport(\n",
    "        skin_lesion=data.report.skin_lesion,\n",
    "        risk=data.report.risk,\n",
    "        conclusion=data.report.conclusion,\n",
    "        skin_lesion_conclusion=data.report.skin_lesion_conclusion,\n",
    "    )\n",
    "\n",
    "    simple_lesion_data = dt.SimpleLesionData(\n",
    "        exam_id=data.exam_id,\n",
    "        image=data.image,\n",
    "        report=simple_report\n",
    "    )\n",
    "\n",
    "    simple_dataset.append(simple_lesion_data)\n",
    "\n",
    "dataset = simple_dataset\n",
    "\n",
    "data_dicts = [data.model_dump() for data in dataset]\n",
    "dataset_name = 'simple_dataset.json'\n",
    "\n",
    "with open(join(defs.DATA_PATH, 'stt_data', dataset_name), 'w', encoding='utf-8') as file:\n",
    "    dump(data_dicts, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "_ = dt.analyse_simple_dataset(simple_dataset, defs.DATA_PATH, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seccionamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erfer/Documents/UFSC/INE5453-33-34_TCC/Code/TCC/experiments/.venv/lib/python3.10/site-packages/sklearn/model_selection/_split.py:1023: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'exam_id': data.exam_id,\n",
    "        'image': data.image,\n",
    "        'skin_lesion': data.report.skin_lesion,\n",
    "        'risk': data.report.risk,\n",
    "        'conclusion': data.report.conclusion,\n",
    "        'skin_lesion_conclusion': data.report.skin_lesion_conclusion,\n",
    "    }\n",
    "    for data in dataset\n",
    "])\n",
    "\n",
    "stratified_split = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=defs.STATIC_RANDOM_STATE)\n",
    "training_idx, test_validation_idx = next(stratified_split.split(df, df['skin_lesion'], groups=df['exam_id']))\n",
    "training_df = df.iloc[training_idx]\n",
    "test_validation_df = df.iloc[test_validation_idx]\n",
    "\n",
    "stratified_split_validation_test = StratifiedGroupKFold(n_splits=2, shuffle=True, random_state=defs.STATIC_RANDOM_STATE)\n",
    "validation_idx, test_idx = next(stratified_split_validation_test.split(test_validation_df,\n",
    "                                                                       test_validation_df['skin_lesion'],\n",
    "                                                                       groups=test_validation_df['exam_id']))\n",
    "validation_df = test_validation_df.iloc[validation_idx]\n",
    "test_df = test_validation_df.iloc[test_idx]\n",
    "\n",
    "training_data = [\n",
    "    dt.SimpleLesionData(\n",
    "        exam_id=row.exam_id,\n",
    "        image=row.image,\n",
    "        report=dt.SimpleReport(\n",
    "            skin_lesion=row.skin_lesion,\n",
    "            risk=row.risk,\n",
    "            conclusion=row.conclusion,\n",
    "            skin_lesion_conclusion=row.skin_lesion_conclusion,\n",
    "        ),\n",
    "    )\n",
    "    for _, row in training_df.iterrows()\n",
    "]\n",
    "\n",
    "validation_data = [\n",
    "    dt.SimpleLesionData(\n",
    "        exam_id=row.exam_id,\n",
    "        image=row.image,\n",
    "        report=dt.SimpleReport(\n",
    "            skin_lesion=row.skin_lesion,\n",
    "            risk=row.risk,\n",
    "            conclusion=row.conclusion,\n",
    "            skin_lesion_conclusion=row.skin_lesion_conclusion,\n",
    "        ),\n",
    "    )\n",
    "    for _, row in validation_df.iterrows()\n",
    "]\n",
    "\n",
    "test_data = [\n",
    "    dt.SimpleLesionData(\n",
    "        exam_id=row.exam_id,\n",
    "        image=row.image,\n",
    "        report=dt.SimpleReport(\n",
    "            skin_lesion=row.skin_lesion,\n",
    "            risk=row.risk,\n",
    "            conclusion=row.conclusion,\n",
    "            skin_lesion_conclusion=row.skin_lesion_conclusion,\n",
    "        ),\n",
    "    )\n",
    "    for _, row in test_df.iterrows()\n",
    "]\n",
    "\n",
    "dataset_pairs = ((training_data, 'training_dataset.json'),\n",
    "                 (test_data, 'test_dataset.json'),\n",
    "                 (validation_data, 'validation_dataset.json'))\n",
    "\n",
    "for dataset, dataset_name in dataset_pairs:\n",
    "    dataset_dict = [data.model_dump() for data in dataset]\n",
    "\n",
    "    with open(join(defs.DATA_PATH, 'stt_data', dataset_name), 'w', encoding='utf-8') as file:\n",
    "        dump(dataset_dict, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    _ = dt.analyse_simple_dataset(dataset, defs.DATA_PATH, dataset_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
