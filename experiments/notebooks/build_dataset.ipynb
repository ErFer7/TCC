{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construção do dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from os import makedirs\n",
    "from json import load, dump\n",
    "from re import search\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import scripts.definitions as defs\n",
    "import scripts.data as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtragem dos IDs de exames de aproximação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(defs.DATA_PATH, 'stt_raw_data', 'dataset', 'dataset.json'), 'r', encoding='utf-8') as file:\n",
    "    dataset = [dt.RawData(**data) for data in load(file)]\n",
    "\n",
    "raw_lesion_dataset: dict[int, list[dt.RawLesionData]] = {}\n",
    "\n",
    "for raw_data in tqdm(dataset, desc='Processando exames: '):\n",
    "    approximation_series = list(filter(lambda series: search(r'LesÃ£o \\d+.', series.seriesdescription),\n",
    "                                       raw_data.series))\n",
    "\n",
    "    if len(approximation_series) == 0:\n",
    "        continue\n",
    "\n",
    "    raw_lesion_dataset[raw_data.id_exame] = []\n",
    "\n",
    "    for series in approximation_series:\n",
    "        series_description = series.seriesdescription.split()\n",
    "        lesion_location = ' '.join(series_description[3:]).encode('latin1').decode('utf-8')\n",
    "\n",
    "        raw_lesion_data = dt.RawLesionData(\n",
    "            exam_id=raw_data.id_exame,\n",
    "            images=series.instances,\n",
    "            lesion_number=int(series_description[1]),\n",
    "            lesion_location=lesion_location,\n",
    "            report=''\n",
    "        )\n",
    "\n",
    "        raw_lesion_dataset[raw_data.id_exame].append(raw_lesion_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversão de CSV para JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(join(defs.DATA_PATH, 'stt_raw_data', 'REDE_QUALIDADE-laudos-reemitidos.csv'))\n",
    "\n",
    "REPLACEMENTS = {\n",
    "    '\\\\n': '\\n',\n",
    "    '<br />': '\\n',\n",
    "    '&emsp;': ' ',\n",
    "    '&lt;': '<',\n",
    "    '&gt;': '>',\n",
    "    '–': '-',\n",
    "}\n",
    "\n",
    "raw_report_dataset = {}\n",
    "\n",
    "for _, raw_report in tqdm(df.iterrows(), desc='Processando laudos: '):\n",
    "    raw_report = dt.RawReport(**raw_report.to_dict())\n",
    "\n",
    "    exam_id = int(raw_report.id_exame)\n",
    "\n",
    "    if exam_id not in raw_lesion_dataset:\n",
    "        continue\n",
    "\n",
    "    report = raw_report.laudo\n",
    "\n",
    "    for pattern, replacement in REPLACEMENTS.items():\n",
    "        report = report.replace(pattern, replacement)\n",
    "\n",
    "    raw_report_dataset[exam_id] = report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geração dos laudos estruturados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = {}\n",
    "\n",
    "for exam_id, raw_report in tqdm(raw_report_dataset.items(), desc='Estruturando laudos: '):\n",
    "    # O [1:] remove o tipo de laudo. Sempre é \"Exame de Teledermatologia\"\n",
    "    report_parts = list(map(str.strip, raw_report.splitlines()))[1:]\n",
    "\n",
    "    parsed_reports = []\n",
    "    footnotes = {}\n",
    "\n",
    "    while True:\n",
    "        report, has_next = dt.parse_report(report_parts)\n",
    "\n",
    "        if not has_next:\n",
    "            footnotes = dt.parse_report_footnote(report_parts)\n",
    "            break\n",
    "\n",
    "        parsed_reports.append(report)\n",
    "\n",
    "    locations: dict[str, list[dt.Report]] = {}\n",
    "\n",
    "    for parsed_report in parsed_reports:\n",
    "        if parsed_report.location not in locations:\n",
    "            locations[parsed_report.location] = []\n",
    "\n",
    "        locations[parsed_report.location].append(parsed_report)\n",
    "\n",
    "    for location, reports in locations.items():\n",
    "        valid = True\n",
    "        reference_report = reports[0]\n",
    "\n",
    "        for report in reports[1:]:\n",
    "            if report.elementary_lesions != reference_report.elementary_lesions or \\\n",
    "               report.secondary_lesions != reference_report.secondary_lesions or \\\n",
    "               report.coloration != reference_report.coloration or \\\n",
    "               report.morphology != reference_report.morphology or \\\n",
    "               report.size != reference_report.size or \\\n",
    "               report.distribution != reference_report.distribution or \\\n",
    "               report.risk != reference_report.risk or \\\n",
    "               report.skin_lesion != reference_report.skin_lesion:\n",
    "                valid = False\n",
    "                break\n",
    "\n",
    "        if not valid:\n",
    "            for report in reports:\n",
    "                parsed_reports.remove(report)\n",
    "            continue\n",
    "\n",
    "    for raw_lesion_data in raw_lesion_dataset[exam_id]:\n",
    "        index = 0\n",
    "\n",
    "        while index < len(parsed_reports):\n",
    "            report = parsed_reports[index]\n",
    "\n",
    "            if raw_lesion_data.lesion_location == report.location:\n",
    "                if raw_lesion_data.report != '':\n",
    "                    raise ValueError('Conflito de laudos')\n",
    "\n",
    "                raw_lesion_data.report = parsed_reports.pop(index)\n",
    "                break\n",
    "\n",
    "            index += 1\n",
    "        \n",
    "        if raw_lesion_data.report != '':\n",
    "            lesion_number = raw_lesion_data.lesion_number\n",
    "            footnote = footnotes.get(lesion_number, None)\n",
    "\n",
    "            if footnote is not None:\n",
    "                formatted_footnote = '\\n\\nConclusão da lesão:\\n' + '\\n'.join(footnote)\n",
    "                raw_lesion_data.report.conclusion += formatted_footnote  # type: ignore\n",
    "lesion_dataset: list[dt.LesionData] = []\n",
    "\n",
    "for exam_id, raw_lesion_datalist in raw_lesion_dataset.items():\n",
    "    for raw_lesion_data in raw_lesion_datalist:\n",
    "        if raw_lesion_data.report != '':\n",
    "            for image in raw_lesion_data.images:\n",
    "                lesion_data = dt.LesionData(exam_id=exam_id, image=image, report=raw_lesion_data.report)  # type: ignore\n",
    "                lesion_dataset.append(lesion_data)\n",
    "\n",
    "approximation_exams_dicts = [approximation_exam.model_dump() for approximation_exam in lesion_dataset]\n",
    "\n",
    "makedirs(join(defs.DATA_PATH, 'stt_data'), exist_ok=True)\n",
    "\n",
    "with open(join(defs.DATA_PATH, 'stt_data', 'dataset.json'), 'w', encoding='utf-8') as file:\n",
    "    dump(approximation_exams_dicts, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "data_analysis = dt.analyse_dataset(lesion_dataset, defs.DATA_PATH, 'dataset.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção de lesões raras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = list(filter(lambda data: data_analysis.skin_lesion_distribution.classes[data.report.skin_lesion].count >= 10,\n",
    "                      lesion_dataset))\n",
    "\n",
    "_ = dt.analyse_dataset(dataset, defs.DATA_PATH, 'filtered_dataset.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seccionamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Resolver o problema dos exames\n",
    "labels = [data.report.skin_lesion for data in dataset]\n",
    "\n",
    "training_data, test_data = train_test_split(\n",
    "    dataset,\n",
    "    test_size=defs.TEST_PROPORTION,\n",
    "    train_size=defs.TRAINING_PROPORTION,\n",
    "    stratify=labels,\n",
    "    random_state=defs.STATIC_RANDOM_STATE\n",
    ")\n",
    "\n",
    "training_labels = [data.report.skin_lesion for data in training_data]\n",
    "\n",
    "_, validation_data = train_test_split(\n",
    "    training_data,\n",
    "    test_size=defs.VALIDATION_PROPORTION / defs.TRAINING_PROPORTION,\n",
    "    stratify=training_labels,\n",
    "    random_state=defs.STATIC_RANDOM_STATE\n",
    ")\n",
    "\n",
    "dataset_pairs = ((training_data, 'training_dataset.json'),\n",
    "                 (test_data, 'test_dataset.json'),\n",
    "                 (validation_data, 'validation_dataset.json'))\n",
    "\n",
    "\n",
    "for dataset, dataset_name in dataset_pairs:\n",
    "    dataset_dict = [data.model_dump() for data in dataset]\n",
    "\n",
    "    with open(join(defs.DATA_PATH, 'stt_data', dataset_name), 'w', encoding='utf-8') as file:\n",
    "        dump(dataset_dict, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    _ = dt.analyse_dataset(dataset, defs.DATA_PATH, dataset_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
